{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Callum-Roberts713/MiscPythonCode/blob/main/DataScienceAssignment1CallumRoberts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V99Gzz7TQUTx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy import genfromtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.base.model import GenericLikelihoodModel\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz-Ca4jxQUT-"
      },
      "source": [
        "Here i started by just importing some of thepackages i will be using in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa9lbob1QUUB",
        "outputId": "1a7d1be7-136d-4e24-e3eb-cb2edd572e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d34d47cc26fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cereal.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shelf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shelf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'carbo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'carbo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'carbo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'carbo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cereal.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('cereal.csv')\n",
        "df['shelf']=df['shelf'].replace(20,2)\n",
        "df['protein']=df['protein'].replace(np.nan,0)\n",
        "df['carbo']=df['carbo'].replace(np.nan,0)\n",
        "df['carbo']=df['carbo'].abs()\n",
        "df['sugars']=df['sugars'].abs()\n",
        "df['potass']=df['potass'].abs()\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfnohySlQUUE"
      },
      "source": [
        "Here i begin by reading the data from the csv file we were given\n",
        "I then started to clean the data, first i noticed one of the values for the shelf column was 20 and since the allowed values were 1,2, or 3 i took the 20 and assumed there was a misinput when writing the initial data and changed that to a 2\n",
        "then i noticed some values written as NaN or \"not a number\" and as that wouldnt plot well later on i changed all NaN values to zero.\n",
        "There were also some negative numbers in some of the columns which didnt makes sense with the data i took the absolute of all columns with negative values in them.\n",
        "Lastly, just to be sure there were no duplications in the data i ran the drop duplicates function\n",
        "and then i reset the index of the file so there would be no issues with reading the data later on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNpjCnTwQUUF"
      },
      "outputs": [],
      "source": [
        "print(df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Cf1vjOQUUG"
      },
      "source": [
        "here i wrote the data to a string so i could check my cleaning had worked for all the values and there were no remaining negative values or misswritten data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CecJXbohQUUH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "b663537c-0065-42b7-9ac3-231d1ef401f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f3fe545020e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mManufacturerNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mfr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mManufacturerNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mManufacturerNumbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mfr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mManufacturerNumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "ManufacturerNames=df['mfr'].unique()\n",
        "print(ManufacturerNames)\n",
        "ManufacturerNumbers=df['mfr'].value_counts()\n",
        "print(ManufacturerNumbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGpy6NmFQUUI"
      },
      "source": [
        "My initial prediction was that manufacturers with more cereal brands would have a higher average rating than manufacturers with less brands. my thinking was that a manufacturer with more brands has more advertising oppourtuunities and therefore more people would be likly to buy their brands of cereal and rate it higher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l00UFXK6QUUJ"
      },
      "outputs": [],
      "source": [
        "df_rating=df.sort_values(by=['rating'], ascending=True).reset_index()\n",
        "df_rating_R=df_rating['rating']\n",
        "df_rating_1=df_rating['sugars']\n",
        "df_rating_2=df_rating['potass']\n",
        "df_rating_3=df_rating['sodium']\n",
        "df_rating_4=df_rating['shelf']\n",
        "df_rating_5=df_rating['mfr']\n",
        "df_rating_6=df_rating['calories']\n",
        "df_rating_7=df_rating['fat']\n",
        "df_rating_8=df_rating['fiber']\n",
        "df_rating_9=df_rating['carbo']\n",
        "df_rating_10=df_rating['protein']\n",
        "df_rating_11=df_rating['vitamins']\n",
        "df_rating_12=df_rating['weight']\n",
        "df_rating_13=df_rating['cups']\n",
        "df_rating\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGFmdQnQUUK"
      },
      "source": [
        "i then reorganised the data so it was in order of rating, because when plotting the data later on it was more coherant if the data was ordered by the rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAkp1-FeQUUL"
      },
      "outputs": [],
      "source": [
        "df_grp = df.groupby(['mfr'])\n",
        "Kellogs=df_grp.get_group('K').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(Kellogs)\n",
        "General_Mills=df_grp.get_group('G').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(General_Mills)\n",
        "Post=df_grp.get_group('P').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(Post)\n",
        "Quaker_Oats=df_grp.get_group('Q').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(Quaker_Oats)\n",
        "Ralston_Purina=df_grp.get_group('K').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(Ralston_Purina)\n",
        "Nabisco=df_grp.get_group('N').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(Nabisco)\n",
        "American_Home_Foods=df_grp.get_group('A').sort_values(by=['rating'], ascending=True).reset_index()\n",
        "print(American_Home_Foods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rku4MU0xQUUM"
      },
      "source": [
        "Then to begin i sorted the data still organised by rating into tables just containing each of the manufacturers. This made it easier to then figure out the average ratings below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDhfR8wQQUUN"
      },
      "outputs": [],
      "source": [
        "KellogsMean=np.mean(Kellogs['rating'])\n",
        "print('Kellogs Mean',KellogsMean)\n",
        "KellogsMedian=np.median(Kellogs['rating'])\n",
        "print('Kellogs Median',KellogsMedian)\n",
        "General_MillsMean=np.mean(General_Mills['rating'])\n",
        "print('General Mills Mean',General_MillsMean)\n",
        "General_MillsMedian=np.median(General_Mills['rating'])\n",
        "print('General Mills Median',General_MillsMedian)\n",
        "PostMean=np.mean(Post['rating'])\n",
        "print('Post Mean',PostMean)\n",
        "PostMedian=np.median(Post['rating'])\n",
        "print('Post Median',PostMedian)\n",
        "Quaker_OatsMean=np.mean(Quaker_Oats['rating'])\n",
        "print('Quaker Oats Mean',Quaker_OatsMean)\n",
        "Quaker_OatsMedian=np.median(Quaker_Oats['rating'])\n",
        "print('Quaker Oats Median',Quaker_OatsMedian)\n",
        "Ralston_PurinaMean=np.mean(Ralston_Purina['rating'])\n",
        "print('Ralston Purina Mean',Ralston_PurinaMean)\n",
        "Ralston_PurinaMedian=np.median(Ralston_Purina['rating'])\n",
        "print('Ralston Purina Median',Ralston_PurinaMedian)\n",
        "NabiscoMean=np.mean(Nabisco['rating'])\n",
        "print('Nabisco Mean',NabiscoMean)\n",
        "NabiscoMedian=np.median(Nabisco['rating'])\n",
        "print('Nabisco Median',NabiscoMedian)\n",
        "American_Home_FoodsMean=np.mean(American_Home_Foods['rating'])\n",
        "print('American Home Foods Mean',American_Home_FoodsMean)\n",
        "American_Home_FoodsMedian=np.median(American_Home_Foods['rating'])\n",
        "print('American Home Foods Median',American_Home_FoodsMedian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eehlS4vWQUUO"
      },
      "source": [
        "Here i used numpy functions to calculate the mean and median rating of each of the manufacturers cereals so that i could plot them later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz1hAV6rQUUP"
      },
      "outputs": [],
      "source": [
        "MeanRating=np.array([KellogsMean,General_MillsMean,PostMean,Quaker_OatsMean,Ralston_PurinaMean, NabiscoMean, American_Home_FoodsMean])\n",
        "print(MeanRating)\n",
        "MedianRating=np.array([KellogsMedian,General_MillsMedian,PostMedian,Quaker_OatsMedian,Ralston_PurinaMedian, NabiscoMedian, American_Home_FoodsMedian])\n",
        "print(MedianRating)\n",
        "Manufacturers=np.array(['Kellogs','General Mills','Post','Quaker Oats', 'Ralston Purina', 'Nabisco', 'American Home Foods'])\n",
        "print(Manufacturers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhQ5If6cQUUQ"
      },
      "source": [
        "Here i made arrays of the mean ratings, the median ratings and the names of the manufacturers, crucially i did this in order of the number of brands each manufacturer made so i could then plot how the \"size\" of a manufacturer compared to its average rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e-aPsEjQUUQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "plt.plot( Manufacturers,ManufacturerNumbers,color='blue')\n",
        "plt.title('Number of brands rated from each manufacturer')\n",
        "plt.xlabel('Manufacturer')\n",
        "plt.ylabel('Number of Brands')\n",
        "plt.figure(2)\n",
        "plt.plot(Manufacturers,MeanRating,color='red',label='mean rating')\n",
        "plt.plot(Manufacturers,MedianRating,color='green', label='median rating')\n",
        "plt.title('Mean and Median rating of each manufacturer')\n",
        "plt.xlabel('Manufacturer')\n",
        "plt.ylabel('Average rating')\n",
        "plt.legend()\n",
        "plt.figure(3)\n",
        "plt.plot(ManufacturerNumbers,MeanRating,color='red',label='mean rating')\n",
        "plt.plot(ManufacturerNumbers,MedianRating,color='green', label='median rating')\n",
        "plt.title('Average rating against number of brands a manufacturer has')\n",
        "plt.xlabel('Number of brands')\n",
        "plt.ylabel('Average rating')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_-XfID-QUUR"
      },
      "source": [
        "here i made three plots:\n",
        "1) the number of brands from each manufacturer in our data, this is really just to get an idea of how the distribution of brands looked\n",
        "2) i then made a graph just comparing the average score of the manufacturers, this is when i first realised that the lack of data from some of the brands might skew the data towards the manufacturers with less brands as they have less data to go off\n",
        "3) this is a graph of the number of brands against the average rating of each of the brands again this conirmed my suspicion as the brands with less data came up with more positive averages due to the fact that they had less data to go off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSaJ_A8MQUUS"
      },
      "outputs": [],
      "source": [
        "KellogsRating=Kellogs['rating']\n",
        "General_MillsRating=General_Mills['rating']\n",
        "PostRating=Post['rating']\n",
        "Quaker_OatsRating=Quaker_Oats['rating']\n",
        "Ralston_PurinaRating=Ralston_Purina['rating']\n",
        "NabiscoRating=Nabisco['rating']\n",
        "American_Home_FoodsRating=American_Home_Foods['rating']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaR_-erSQUUS"
      },
      "source": [
        "despite my suspicions i wanted to check by making histograms of the averages of the each of the manufacturers so i started by seperating the data into the ratings column of each of the manufacturers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQhEC7xQUUS"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "plt.hist(KellogsRating,color='red',bins=15)\n",
        "plt.figure(2)\n",
        "plt.hist(General_MillsRating,color='green',bins=15)\n",
        "plt.figure(3)\n",
        "plt.hist(PostRating,color='blue',bins=15)\n",
        "plt.figure(4)\n",
        "plt.hist(Quaker_OatsRating,color='yellow',bins=15)\n",
        "plt.figure(5)\n",
        "plt.hist(Ralston_PurinaRating,color='orange',bins=10)\n",
        "plt.figure(6)\n",
        "plt.hist(NabiscoRating,color='purple',bins=10)\n",
        "plt.figure(7)\n",
        "plt.hist(American_Home_FoodsRating,color='pink',bins=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw1tKBd-QUUT"
      },
      "source": [
        "again this just went towards confirming my suspicions as there was no clear corrolation between the manufacturer and the ratings of each of the brands, however it was interesting to see how the rating of each of the brands looked as a histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-mVdqkUQUUT"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "plt.plot(df_rating_R,df_rating_1)\n",
        "plt.title('Rating against Sugars')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Sugars')\n",
        "plt.figure(2)\n",
        "plt.plot(df_rating_R,df_rating_2,color='red')\n",
        "plt.title('Rating against Potassium')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Potassium')\n",
        "plt.figure(3)\n",
        "plt.plot(df_rating_R,df_rating_3,color='yellow')\n",
        "plt.title('Rating against Sodium')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Sodium')\n",
        "plt.figure(4)\n",
        "plt.plot(df_rating_R,df_rating_4,color='green')\n",
        "plt.title('Rating against Shelf')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Shelf')\n",
        "plt.figure(5)\n",
        "plt.plot(df_rating_R,df_rating_6,color='black')\n",
        "plt.title('Rating against Calories')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Calories')\n",
        "plt.figure(6)\n",
        "plt.plot(df_rating_R,df_rating_7,color='orange')\n",
        "plt.title('Rating against Fat')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Fat')\n",
        "plt.figure(7)\n",
        "plt.plot(df_rating_R,df_rating_8,color='purple')\n",
        "plt.title('Rating against Fiber')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Fiber')\n",
        "plt.figure(8)\n",
        "plt.plot(df_rating_R,df_rating_9,color='pink')\n",
        "plt.title('Rating against Carbs')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Carbs')\n",
        "plt.figure(9)\n",
        "plt.plot(df_rating_R,df_rating_10,color='cyan')\n",
        "plt.title('Rating against Protein')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Protein')\n",
        "plt.figure(10)\n",
        "plt.plot(df_rating_R,df_rating_11,color='grey')\n",
        "plt.title('Rating against Vitamins')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Vitamins')\n",
        "plt.figure(11)\n",
        "plt.plot(df_rating_R,df_rating_12,color='indigo')\n",
        "plt.title('Rating against Cups')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Cups')\n",
        "plt.figure(12)\n",
        "plt.plot(df_rating_R,df_rating_13,color='teal')\n",
        "plt.title('Rating against Weight')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Weight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCLpWOKqQUUU"
      },
      "source": [
        "after my hypothesis failed i wanted to see if there was a correlation between any of the other data and the ratings so i plotted the data of the ratings against each of the other columns , there was alot of noise in the data as there were not many data points so just by eye it was difficult to see if there was much correlation based off just the plots other than the sugars against ratings and the calories against ratings there was no real discernable correlations between the data and the ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoHGR_jcQUUV"
      },
      "outputs": [],
      "source": [
        "correlationsPearson=df_rating.corr()\n",
        "print(correlationsPearson)\n",
        "correlationsSpearman=df_rating.corr(method=\"spearman\")\n",
        "print(correlationsSpearman)\n",
        "correlationsKendall=df_rating.corr(method=\"kendall\")\n",
        "print(correlationsKendall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW-3TZvOQUUV"
      },
      "source": [
        "To check numerically weather there was correlations or not i used pythons .corr functions to gather the R-Values for each of the values in the data, i used all three methods to calculate the R-value, pearsons, spearmans and kendalls. i suspected that the values for pearsons would be the best to look at as the correlations were mostly linear however i wanted to see how different the methods would look when visualised.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlbxzhaMQUUW"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "sns.heatmap(correlationsPearson)\n",
        "plt.figure(2)\n",
        "sns.heatmap(correlationsSpearman)\n",
        "plt.figure(3)\n",
        "sns.heatmap(correlationsKendall)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMdshvstQUUW"
      },
      "source": [
        "To visualise how the correlations looked i created a heatmap of all the values, since R-Value is a measure esentially of the percentage of correlation in both the positive and negative directions it seemed useful to beable to visualise them all on three graphs with colour as a ditinguishing factor. as i predicted earlier from the graphs there were strong negative correlations between sugars, calories and ratings. however from the visualisation i can see that there are also some correlations between the protein and ratings and the fiber and ratings which was interesting to see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoOi4_aOQUUW"
      },
      "outputs": [],
      "source": [
        "e=np.arange(0,77,1)\n",
        "print(e)\n",
        "StandardDeviation=np.std(df_rating_R)\n",
        "MeanRatingT=np.mean(df_rating_R)\n",
        "print(StandardDeviation)\n",
        "print(MeanRatingT)\n",
        "gausian=np.random.normal(MeanRatingT,StandardDeviation,15)\n",
        "plt.figure()\n",
        "plt.hist(df_rating_R,bins=15)\n",
        "plt.plot(gausian)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjA15-9LQUUX"
      },
      "source": [
        "i first plotted a gausian distribution across the histogram of the ratings but the distribution didnt fit the data at all so i decided to look for some other functions that would give me some numerical values to check the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqfs8fbmQUUX"
      },
      "outputs": [],
      "source": [
        "liklihood=sm.OLS(df_rating_R,e).fit()\n",
        "print(liklihood.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3cwxj-lQUUX"
      },
      "source": [
        "i started with a distribution called OLS orordinary least squares, i didnt have high hopes for this as its mainly used for linear distributions but i wanted to check first it gave me some values for the liklihood, aic and bic although as i thought they were not very strong values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZzBfAk9QUUX"
      },
      "outputs": [],
      "source": [
        "sm.NegativeBinomial(df_rating_R,e).fit().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ7dfF0OQUUX"
      },
      "source": [
        "i then checked the negative binomial distribution which is a method of checking a poisson distribution and due to the poisson distribution being a curve this one was much more promising as the log liklihood was much more negative than the OLS values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7Tzkgk7QUUY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXCnd8r1QUUY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}